gansu[9,4] = mean(gansu_30$expend_per_capita)
gansu[10,4] = sd(gansu_30$expend_per_capita)
# calories per capita
gansu[11,4] = mean(gansu_30$hh_cals_percap)
gansu[12,4] = sd(gansu_30$hh_cals_percap)
# rice per capita
gansu[13,4] = mean(gansu_30$hh_rice_percap)
gansu[14,4] = sd(gansu_30$hh_rice_percap)
# meat per capita
gansu[15,4] = mean(gansu_30$hh_meat_percap)
gansu[16,4] = sd(gansu_30$hh_meat_percap)
# rice calorie share
gansu[17,4] = mean(gansu_30$hh_rice_calorie_share)
gansu[18,4] = sd(gansu_30$hh_rice_calorie_share)
# observations
gansu[19,4] = nrow(gansu_30)
gansu = round(gansu, 2)
gansu[18,4] = sd(gansu_30$hh_wheat_noodles_calorie_share)
# rice calorie share
gansu[17,4] = mean(gansu_30$hh_wheat_noodles_calorie_share)
# rice calorie share
gansu[17,2] = mean(gansu_10$hh_wheat_noodles_calorie_share)
gansu[18,2] = sd(gansu_10$hh_wheat_noodles_calorie_share)
# wheat calorie share
gansu[17,3] = mean(gansu_20$hh_wheat_noodles_calorie_share)
gansu[18,3] = sd(gansu_20$hh_wheat_noodles_calorie_share)
# rice calorie share
gansu[17,1] = mean(gansu_control$hh_wheat_noodles_calorie_share)
gansu[18,1] = sd(gansu_control$hh_wheat_noodles_calorie_share)
View(gansu)
gansu = round(gansu, 2)
gansu[3,1] = 0.56
gansu[4,1] = 0.64
gansu[3,2] = 0.55
gansu[3,3] = 0.54
gansu[3,4] = 0.54
gansu[4,2] = 0.69
gansu[4,2] = 0.66
gansu[4,2] = 0.69
gansu[4,3] = 0.66
gansu[4,4] = 0.60
gansu
View(jm_replication)
jb_df = read.csv("bags_r.csv")
View(jb_df)
jb_df = na.omit(jb_df)
x = c(3, 12, 6, 20, 14)
y = c(60, 35, 55, 15, 15)
xbar = mean(x)
ybar = mean(y)
xy = (x-xbar)*(y-ybar)
x2 = (x-xbar)**2
b1 = sum(xy)/sum(x2)
b0 = ybar - (b1*xbar)
ystar = b0 + (b1*7)
y2 = (y-ybar)**2
sst = sum(y2)
y_expected = c()
y_expected = c(y_expected, b0 + b1*i)
for (i in x)
y_expected = c(y_expected, b0 + b1*i)
ssr_y =(y_expected - ybar)**2
ssr = sum(ssr_y)
sse = sst - ssr
n = 5
s2 = sse/(n-2)
s2_yexpected = s2*((1/n)+ ((7-xbar)**2)/sum(x2))
estimated_s = sqrt(s2)/sqrt(sum(x2))
s_b1 = sqrt(s2)/sqrt(sum(x2))
t = b1/s_b1
lower_bound = b1 - (t*s_b1)
upper_bound = b1+(t*s_b1)
list(lower_bound, upper_bound)
s_pred = sqrt(s2 + s2_yexpected)
7 - (t*s_pred)
7 + (t*s_pred)
x = c(3, 12, 6, 20, 14)
y = c(60, 35, 55, 15, 15)
xbar = mean(x)
ybar = mean(y)
xy = (x-xbar)*(y-ybar)
x2 = (x-xbar)**2
b1 = sum(xy)/sum(x2)
b0 = ybar - (b1*xbar)
ystar = b0 + (b1*7)
y2 = (y-ybar)**2
sst = sum(y2)
y_expected = c()
for (i in x)
y_expected = c(y_expected, b0 + b1*i)
ssr_y =(y_expected - ybar)**2
ssr = sum(ssr_y)
sse = sst - ssr
n = 5
s2 = sse/(n-2)
s2_yexpected = s2*((1/n)+ ((7-xbar)**2)/sum(x2))
sqrt(s2_yexpected)
s_b1 = sqrt(s2)/sqrt(sum(x2))
t = b1/s_b1
lower_bound = b1 - (t*s_b1)
upper_bound = b1+(t*s_b1)
list(lower_bound, upper_bound)
lower_bound = b1 - (3.1824*s_b1)
upper_bound = b1+(3.1824*s_b1)
list(lower_bound, upper_bound)
lower = ystar + (3.1824*s_pred)
s_pred = sqrt(s2 + s2_yexpected)
lower = ystar + (3.1824*s_pred)
upper = ystar - (3.1824*s_pred)
list(lower, upper)
lower_bound = b1 - (3.1824*sqrt(s2_yexpected))
upper_bound = b1+(3.1824*sqrt(s2_yexpected))
x = c(3, 12, 6, 20, 14)
y = c(60, 35, 55, 15, 15)
xbar = mean(x)
ybar = mean(y)
xy = (x-xbar)*(y-ybar)
x2 = (x-xbar)**2
b1 = sum(xy)/sum(x2)
b0 = ybar - (b1*xbar)
ystar = b0 + (b1*7)
y2 = (y-ybar)**2
sst = sum(y2)
y_expected = c()
for (i in x)
y_expected = c(y_expected, b0 + b1*i)
ssr_y =(y_expected - ybar)**2
ssr = sum(ssr_y)
sse = sst - ssr
n = 5
s2 = sse/(n-2)
s2_yexpected = s2*((1/n)+ ((7-xbar)**2)/sum(x2))
s_b1 = sqrt(s2)/sqrt(sum(x2))
lower_bound = b1 - (3.1824*sqrt(s2_yexpected))
upper_bound = b1+(3.1824*sqrt(s2_yexpected))
list(lower_bound, upper_bound)
s_pred = sqrt(s2 + s2_yexpected)
lower = ystar + (3.1824*s_pred)
upper = ystar - (3.1824*s_pred)
list(lower, upper)
0.8951/0.1490
x = c(6, 11, 15, 18, 20)
y = c(5, 9, 12, 21, 30)
xbar = mean(x)
ybar = mean(y)
xy = (x-xbar)*(y-ybar)
x2 = (x-xbar)**2
b1 = sum(xy)/sum(x2)
b0 = ybar - (b1*xbar)
y_expected = c()
for (i in x)
y_expected = c(y_expected, b0 + b1*i)
diff = y - y_expected
list(diff)
standardized = diff/sqrt(y_expected)
list(standardized)
ssr_y =(y_expected - ybar)**2
ssr = sum(ssr_y)
sse = sst - ssr
n = 5
sst = sum(y2)
ssr_y =(y_expected - ybar)**2
y2 = (y-ybar)**2
sst = sum(y2)
ssr_y =(y_expected - ybar)**2
ssr = sum(ssr_y)
sse = sst - ssr
n = 5
s = sqrt(sse/(n-2))
s_b1 = s/sqrt(sum(x2))
standardized = diff/s_b1
list(standardized)
h = (1/n) + x2/sum(x2)
s_diff = s*sqrt(1 - h)
standardized = diff/s_diff
standardized
y = c(145, 100, 120, 115, 130, 130, 105)
x = c(135, 110, 130, 145, 175, 160, 120)
xbar = mean(x)
ybar = mean(y)
xy = (x-xbar)*(y-ybar)
x2 = (x-xbar)**2
b1 = sum(xy)/sum(x2)
b0 = ybar - (b1*xbar)
y_expected = c()
for (i in x)
y_expected = c(y_expected, b0 + b1*i)
diff = y - y_expected
y2 = (y-ybar)**2
sst = sum(y2)
ssr_y =(y_expected - ybar)**2
ssr = sum(ssr_y)
sse = sst - ssr
n = 7
s = sqrt(sse/(n-2))
h = (1/n) + x2/sum(x2)
s_diff = s*sqrt(1 - h)
standardized = diff/s_diff
list(standardized)
matplot(x, y, type="p")
for i in c(0,26):
letters = c('a', 'b', 'c', 'd', 'e', 'f',
'g', 'h', 'i', 'j', 'k', 'l',
'm', 'n', 'o', 'p', 'q', 'r',
's', 't', 'u', 'v', 'w', 'x',
'y', 'z')
letter_freq = data.frame(letter = letters, frequency = 0)
View(letter_freq)
letters = array(0, c(26))
count_letters("thesearewordsthisisthephrase")
count_letters = function(phrase) {
for (char in strsplit(my_string, NULL)[[1]]) {
if (char %in% letters) {
letter_freq$frequency[letter_freq$letter == char] <- letter_freq$frequency[letter_freq$letter == char] + 1
}
}
}
count_letters("thesearewordsthisisthephrase")
if (char %in% letters) {
letter_freq$frequency[letter_freq$letter == char] <- letter_freq$frequency[letter_freq$letter == char] + 1
}
numbers = array(0, c(26))
library(haven)
jm_replication = read_dta("Giffen.dta")
View(jm_replication)
#subset household
jm_hh = subset(jm_replication, person_id == min_p)
hunan = subset(jm_hh, province == "Hunan")
gansu = subset(jm_hh, province == "Gansu")
#Regress the y variable onto x
model = lm(Y ~ X, data = jm_replication)
View(hunan)
quantity_rice = hunan$hh_rice_percap
price_rice = hunan$pct_ch_sub_ri~c
price_rice = hunan$pct_ch_sub_rice_arc
hunan_model = lm(quantity_rice~price_rice, data = hunan)
summary(hunan_model)
library(haven)
jm_replication = read_dta("Giffen.dta")
View(jm_replication)
#subset household
jm_hh = subset(jm_replication, person_id == min_p)
hunan = subset(jm_hh, province == "Hunan")
gansu = subset(jm_hh, province == "Gansu")
#consider relationship on how mpg is predicted by cylinders
data = mtcars
model = lm(mpg~cyl, data = data)
summary(model)
View(jm_hh)
quantity_rice = hunan$pct_ch_hh_rice
price_rice = hunan$pct_ch_sub_rice_arc
hunan_model = lm(quantity_rice~price_rice, data = hunan)
summary(hunan_model)
b0 = -0.09469: Y-intercept of the demand curve.
library(haven)
jm_replication = read_dta("Giffen.dta")
View(jm_replication)
#subset household
jm_hh = subset(jm_replication, person_id == min_p)
hunan = subset(jm_hh, province == "Hunan")
gansu = subset(jm_hh, province == "Gansu")
quantity_rice = hunan$pct_ch_hh_rice
price_rice = hunan$pct_ch_sub_rice_arc
hunan_model = lm(quantity_rice~price_rice, data = hunan)
summary(hunan_model)
quantity_rice = gansu$pct_ch_hh_wheat
quantity_rice = hunan$pct_ch_hh_rice
price_rice = hunan$pct_ch_sub_rice_arc
hunan_model = lm(quantity_rice~price_rice, data = hunan)
summary(hunan_model)
quantity_wheat = gansu$pct_ch_hh_wheat
price_wheat = gansu$pct_ch_sub_wheat_arc
gansu_model = lm(quantity_wheat~price_wheat, data = gansu)
summary(gansu_model)
#consider relationship on how mpg is predicted by cylinders
data = mtcars
model = lm(mpg~cyl, data = data)
summary(model)
quantity_rice = hunan$pct_ch_hh_rice
price_rice = hunan$pct_ch_sub_rice_arc
hunan_model = lm(quantity_rice~price_rice, data = hunan)
summary(hunan_model)
quantity_wheat = gansu$pct_ch_hh_wheat
price_wheat = gansu$pct_ch_sub_wheat_arc
gansu_model = lm(quantity_wheat~price_wheat, data = gansu)
summary(gansu_model)
x1 = c(30, 47, 25, 51, 40, 51, 74, 36, 59, 76)
x2 = c(12, 10, 17, 16, 5, 19, 7, 12, 13, 16)
y = c(94, 108, 112, 178, 94, 175, 170, 117, 142, 211)
dataset = {x1, x2, y}
dataset = c(x1, x2, y)
dataset = data.frame(x1, x2, y)
View(dataset)
gansu_model = lm(y~ x1 + x2, data = gansu)
gansu_model = lm(y~ x1 + x2, data = dataset)
model = lm(y~ x1 + x2, data = dataset)
summary(model)
new.dat <- data.frame(speed=30)
View(new.dat)
new.dat <- data.frame(x1=51, x2 = 16)
View(new.dat)
predict(model, newdata = new.dat, interval = 'confidence')
predict(model, newdata = new.dat, interval = 'prediction')
install.packages(c("class", "dbplyr", "evaluate", "fansi", "foreign", "ggplot2", "htmltools", "KernSmooth", "knitr", "lattice", "markdown", "MASS", "Matrix", "mgcv", "nlme", "nnet", "prettyunits", "ragg", "rpart", "spatial", "survival", "systemfonts", "textshaping", "tinytex", "utf8", "vctrs", "vroom", "withr", "xfun"))
getwd()
nfl_df = read.csv("/Users/dhivyachari/Desktop/econ203/nflpassing_r.csv")
total_pts = 1000
lab_pts = 120
exam_pts = 360
mp_pts = 360
final_pts = 160
final_pts + mp_pts + exam_pts + lab_pts
exam1 = .80
exam_scores = c(.8, .95, 1, .93, .72)
mp_scores = c(.9, .97, .92, .95, 1)
exam_pct = sum(exam_scores * exam_pts)
exam_pct = sum(exam_scores * 60)
mp_pct = sum(mp_scores * 60)
total_pts = 300 + 300 + 120 + 160
total_pts = 300 + 300 + 120 + 160
lab_pts = 120
exam_scores = c(.8, .95, 1, .93, .72)
mp_scores = c(.9, .97, .92, .95, 1)
exam_pts = sum(exam_scores * 60)
mp_pts = sum(mp_scores * 60)
total = (exam_pct + mp_pct + lab_pts)/total_pts
total = (exam_pts + mp_pts + lab_pts)/total_pts
total = exam_pts + mp_pts + lab_pts
extra_credit = 19*(1) + 6.72
total += extra_credit
total = total+extra_credit
total_pts = 300 + 300 + 120 + 160
## 1000 - 120 remaining points
final_pts = 160
lab_pts = 120
exam_scores = c(.8, .95, 1, .93, .72)
mp_scores = c(.9, .97, .92, .95, 1)
exam_pts = sum(exam_scores * 60)
mp_pts = sum(mp_scores * 60)
total = exam_pts + mp_pts + lab_pts
total = exam_pts + mp_pts + lab_pts+extra_credit
extra_credit = 19*(1) + 6.72
total = exam_pts + mp_pts + lab_pts+extra_credit
target_pts = total_pts*0.94
827/880
disparity = (target-total)
disparity = (target_pts-total)
percentage_needed_on_final = disparity/160
# 19 potds as of 11/8
extra_credit = 31*(1) + 6.72
total = exam_pts + mp_pts + lab_pts+extra_credit
target_pts = total_pts*0.94
disparity = (target_pts-total)
percentage_needed_on_final = disparity/160
19
# 19 potds as of 11/8
extra_credit = 19*(1) + 6.72
total = exam_pts + mp_pts + lab_pts+extra_credit
target_pts = total_pts*0.94
disparity = (target_pts-total)
percentage_needed_on_final = disparity/160
setwd("/Users/dhivyachari/caesar_cipher")
library(ggplot2)
letters = c('a', 'b', 'c', 'd', 'e', 'f',
'g', 'h', 'i', 'j', 'k', 'l',
'm', 'n', 'o', 'p', 'q', 'r',
's', 't', 'u', 'v', 'w', 'x',
'y', 'z')
## Importing observed frequency data on guess (letters count csv file)
observed_df = data.frame(letter = letters, frequency = 0)
counts_df = read.csv("lettercounts.csv")
View(counts_df)
View(counts_df)
observed_df["frequency"] = counts_df$Counts
ggplot(observed_df, aes(x=letters, y=frequency)) +
geom_bar(stat = "identity")
## Creating dataset for expected percent frequencies
expected_pct_df <- data.frame (
Letters = c('a', 'b', 'c', 'd', 'e', 'f',
'g', 'h', 'i', 'j', 'k', 'l',
'm', 'n', 'o', 'p', 'q', 'r',
's', 't', 'u', 'v', 'w', 'x',
'y', 'z'),
Frequency = c(8.2, 1.5, 2.8, 4.3, 12.7,
2.2, 2.0, 6.1, 7.0, 0.15,
0.77, 4, 2.4, 6.7, 7.5,
1.9, .095, 6, 6.3, 9.1,
2.8, 0.98, 2.4, 0.15, 2.0, .074)
)
### changing values to percentages
expected_pct_df$Frequency = expected_pct_df$Frequency/100
View(expected_pct_df)
num_letters = sum(observed_df$frequency)
## gives expected count of each letter in a phrase as long as the input
expected_df = data.frame(expected_pct_df$Frequency*num_letters)
colnames(expected_df) <- c("frequency")
View(expected_df)
observed = observed_df$frequency
expected = expected_df$frequency
obs_minus_expect_df = data.frame(diff = observed - expected)
View(obs_minus_expect_df)
## solving for chi square value
obs_minus_expect_df$diff = obs_minus_expect_df$diff^2
obs_minus_expect_df$diff = obs_minus_expect_df$diff/expected
chi_sq_val = sum(obs_minus_expect_df$diff)
View(observed_df)
View(observed_df)
View(obs_minus_expect_df)
View(expected_df)
View(observed_df)
View(expected_df)
View(obs_minus_expect_df)
## solving for chi square value
chi_sq = chisq.test(observed_df$frequency, p = expected_df$frequency)
library(ggplot2)
letters = c('a', 'b', 'c', 'd', 'e', 'f',
'g', 'h', 'i', 'j', 'k', 'l',
'm', 'n', 'o', 'p', 'q', 'r',
's', 't', 'u', 'v', 'w', 'x',
'y', 'z')
## Importing observed frequency data on guess (letters count csv file)
observed_df = data.frame(letter = letters, frequency = 0)
counts_df = read.csv("lettercounts.csv")
observed_df["frequency"] = counts_df$Counts
ggplot(observed_df, aes(x=letters, y=frequency)) +
geom_bar(stat = "identity")
## Creating dataset for expected percent frequencies
expected_pct_df <- data.frame (
Letters = c('a', 'b', 'c', 'd', 'e', 'f',
'g', 'h', 'i', 'j', 'k', 'l',
'm', 'n', 'o', 'p', 'q', 'r',
's', 't', 'u', 'v', 'w', 'x',
'y', 'z'),
Frequency = c(8.2, 1.5, 2.8, 4.3, 12.7,
2.2, 2.0, 6.1, 7.0, 0.15,
0.77, 4, 2.4, 6.7, 7.5,
1.9, .095, 6, 6.3, 9.1,
2.8, 0.98, 2.4, 0.15, 2.0, .074)
)
num_letters = sum(observed_df$frequency)
View(observed_df)
observed_df$frequency = observed_df$frequency/num_letters
sum(frequency)
sum(observed_df$frequency)
ggplot(observed_df, aes(x=letters, y=frequency)) +
geom_bar(stat = "identity")
## Creating dataset for expected percent frequencies
expected_pct_df <- data.frame (
Letters = c('a', 'b', 'c', 'd', 'e', 'f',
'g', 'h', 'i', 'j', 'k', 'l',
'm', 'n', 'o', 'p', 'q', 'r',
's', 't', 'u', 'v', 'w', 'x',
'y', 'z'),
Frequency = c(8.2, 1.5, 2.8, 4.3, 12.7,
2.2, 2.0, 6.1, 7.0, 0.15,
0.77, 4, 2.4, 6.7, 7.5,
1.9, .095, 6, 6.3, 9.1,
2.8, 0.98, 2.4, 0.15, 2.0, .074)
)
### changing values to percentages
expected_pct_df$Frequency = expected_pct_df$Frequency/100
View(expected_pct_df)
observed_pct_df = observed_df$frequency/num_letters
observed_pct_df = data.frame(Frequency=observed_df$frequency/num_letters)
View(observed_pct_df)
observed_df["frequency"] = counts_df$Counts
observed_pct_df = data.frame(Frequency=observed_df$frequency/num_letters)
letters = c('a', 'b', 'c', 'd', 'e', 'f',
'g', 'h', 'i', 'j', 'k', 'l',
'm', 'n', 'o', 'p', 'q', 'r',
's', 't', 'u', 'v', 'w', 'x',
'y', 'z')
## Importing observed frequency data on guess (letters count csv file)
observed_df = data.frame(letter = letters, frequency = 0)
counts_df = read.csv("lettercounts.csv")
observed_df["frequency"] = counts_df$Counts
num_letters = sum(observed_df$frequency)
observed_pct_df = data.frame(Frequency=observed_df$frequency/num_letters)
View(observed_pct_df)
View(observed_df)
81/722
ggplot(observed_df, aes(x=letters, y=frequency)) +
geom_bar(stat = "identity")
### changing values to percentages
expected_pct_df$Frequency = expected_pct_df$Frequency/100
## Creating dataset for expected percent frequencies
expected_pct_df <- data.frame (
Letters = c('a', 'b', 'c', 'd', 'e', 'f',
'g', 'h', 'i', 'j', 'k', 'l',
'm', 'n', 'o', 'p', 'q', 'r',
's', 't', 'u', 'v', 'w', 'x',
'y', 'z'),
Frequency = c(8.2, 1.5, 2.8, 4.3, 12.7,
2.2, 2.0, 6.1, 7.0, 0.15,
0.77, 4, 2.4, 6.7, 7.5,
1.9, .095, 6, 6.3, 9.1,
2.8, 0.98, 2.4, 0.15, 2.0, .074)
)
### changing values to percentages
expected_pct_df$Frequency = expected_pct_df$Frequency/100
chi_sq = chisq.test(observed_pct_df$Frequency, p = expected_pct_df$Frequency)
chi_sq = chisq.test(observed_pct_df$Frequency, expected_pct_df$Frequency)
summary(chi_sq)
## gives expected count of each letter in a phrase as long as the input
expected_df = data.frame(expected_pct_df$Frequency*num_letters)
colnames(expected_df) <- c("frequency")
observed = observed_df$frequency
expected = expected_df$frequency
obs_minus_expect_df = data.frame(diff = observed - expected)
obs_minus_expect_df$diff = obs_minus_expect_df$diff^2
obs_minus_expect_df$diff = obs_minus_expect_df$diff/expected
chi_sq_val = sum(obs_minus_expect_df$diff)
